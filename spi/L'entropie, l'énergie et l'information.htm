<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
  <meta content="text/html; charset=windows-1252" http-equiv="content-type"><title>L'entropie, l'énergie et l'information</title>
  
  <meta content="Jean Zin" name="author"></head>
<body vlink="#000000" text="#000000" link="#009900" bgcolor="#ffffff" alink="#ff0000">
<h2>L'entropie, l'énergie et l'information<br>
</h2>

<div align="justify"><blockquote>
  <blockquote><small><br>
Le terme <i>entropie</i> a été forgé en 1865 par le physicien allemand Clausius à partir de la racine grecque <i>tropi</i>
qui évoque l'idée de transformation ou de retour en
arrière. Il introduisit cette grandeur afin de
caractériser mathématiquement
l'irréversibilité de processus physiques tels qu'une
transformation de travail en chaleur. Depuis cette époque, tel
le dieu grec Protée, le concept d'entropie n'a cessé de
se métamorphoser [...] Nous verrons en particulier que la
notion de <i>quantité d'information</i>, utile en
théorie de la communication ou en informatique, est
étroitement apparentée à celle d'entropie. <br>
    </small>
    <div align="right"><small>Roger Balian, <i>Les états de la matière</i>, p205, Université de tous les savoirs, Odile Jacob, 2002</small><br>
    </div>
  </blockquote>
  <br>Quoi de plus fondamental, de plus important à comprendre que le phénomène de l'entropie? L'entropie
c'est que tout se dégrade, tout fout le camp, c'est la rouille
qui nous ronge, la vieillesse qui nous gagne, le feu qui
s'éteint, le temps qui s'enfuit. L'entropie, c'est que tout a
une fin, c'est la durée en tant que telle, le temps qu'il nous
reste... A l'origine, l'entropie désigne le
phénomène de l'échange thermique qui
égalise les températures ou la dissipation de
l'énergie en chaleur, mais c'est une loi bien plus
générale et qui n'est pas si facile à interpréter. On peut dire que l'entropie c'est que
"tout passe, tout lasse, tout casse", c'est ce qui rend le <b>temps irréversible</b>,
les dégradations et les pertes d'informations
irréparables, la mort certaine, mais, justement, il faut bien
constater que la vie c'est ce
qui résiste à l'entropie, grâce
à l'information comme nous le verrons. La croissance de
l'entropie n'est donc pas une
loi tout-à-fait universelle mais
seulement une
probabilité statistique très grande qui admet des exceptions locales, ce qui
n'est pas marginal pour autant mais absolument essentiel pour les
phénomènes biologiques et constitue la valeur même
de
l'information. La confusion la plus grande règne dans
l'interprétation de l'entropie assimilée aussi bien
à l'information qu'à l'énergie, avec des conséquences politiques désastreuses. Pour tenter de
dissiper cette confusion il faut d'abord
revenir
sur
l'origine des principes de la thermodynamique.<br>
<br>
  <br>
  <b>Les principes de la thermodynamique</b><br>
  
  <div align="right"><a href="http://www.thermodynamique.com/entropi.html"><small>
http://www.thermodynamique.com/entropi.html</small></a><br>
  </div>


<br>
Il y a un "<b>principe zéro</b>" de la thermodynamique qui
stipule que deux systèmes thermodynamiques en équilibre
avec un troisième sont en équilibre entre eux. Leur
propriété commune est la température. Cela veut
dire aussi que pour qu'il y ait transfert d'énergie, il faut une
différence de température (on comprend que
l'analogie de la chaleur avec un flux coulant d'un niveau à
l'autre sous l'effet de la gravitation se soit imposé d'abord.). C'est un principe
mathématique de commutativité et de transitivité.<br>
  <br>
Le <b>premier principe</b>
introduit le dogme de la conservation
d'énergie et de l'équivalence entre travail (W) et
chaleur (Q) mesurés en Joules (J). Une turbine ou un piston
transforment de la chaleur en travail. Dans un système
fermé le bilan énergétique (W+Q=0) est nul par définition, ce qui rend
impossible tout mouvement
perpétuel sans apport externe d'énergie dès lors
qu'il y a travail. C'est pour l'énergie qu'est absolument vrai
qu'une augmentation d'un côté se traduit par une
diminution de l'autre. L'énergie
se conserve (mécanique, thermique, chimique,
électromagnétique), elle ne peut être ni
créé, ni détruite mais elle se transforme. C'est
à ce niveau que "<i>rien ne se perd, rien ne se crée, tout
se transforme</i>"
(Lavoisier). C'est un dogme intangible qui se
réduit à exiger l'égalité entre la cause et
l'effet, entre les deux côtés des équations,
fondant la possibilité de la mathématisation de la
physique (et en premier lieu de la dynamique). L'énergie se définit ainsi comme une constante
numérique, ce qui implique de toujours chercher dans un bilan
énergétique où est passée l'énergie
manquante
(sous forme de chaleur et de rayonnement). Car si l'énergie se
conserve, elle se transforme aussi et dans sa transformation il y a de
la perte, il n'y a jamais transformation intégrale dans une
seule forme d'énergie mais toujours des fuites plus ou moins
importantes dans les autres formes, ce qui débouche sur
le second principe.<br>
  <br>
Le <b>second principe</b>
est celui qui nous intéresse, car, si
le premier principe énonce une conservation de l'énergie
qui semble en contradiction avec notre expérience quotidienne,
le second confirme qu'il y a toujours des phénomènes de
frottement, de diffusion, que l'on considère comme des pertes
d'énergie utilisable, c'est-à-dire qu'on peut transformer en travail. L'énergie ne se conserve pas dans
sa forme originelle mais se transforme et se divise. On
expérimente constamment que
l'entropie d'un système isolé augmente,
c'est-à-dire que
l'énergie, bien que toujours présente, y est de moins en
moins concentrée et disponible. Seule la différence de
niveau d'énergie (de température) est utilisable,
différence qui tend à s'estomper, à revenir
à l'équilibre d'une moyenne en se dissipant ou en se
mélangeant, en perdant la barrière qui la retenait.
Ce que le second principe énonce c'est que
l'entropie n'est pas l'énergie. Ce qu'on perd ce n'est pas de
l'énergie mais seulement ses différences de
répartition, la rigidité d'un ordre, la chaleur produite (l'entropie étant
à l'origine "la
quantité totale de chaleur ajoutée, divisée par la
température").<br>
  <br>
Le <b>troisième principe</b>, ou principe de Nernst,
précise simplement que l'entropie massique devient nulle au
voisinage du zéro absolu qui fige tout mouvement (du moins pour un cristal parfait), ce qui
constitue une base de calcul pour l'entropie dont on ne peut mesurer
sinon qu'une différence d'entropie (constituée elle-même de différences).<br>
  <br>
Le seul principe physique et dynamique, c'est le second principe, dit
aussi principe d'évolution, les
autres étant purement formels. On peut même
considérer comme Einstein que c'est le principe le plus
important de la physique. En tout cas, c'est l'existence de l'<b>entropie</b>
qui pose problème, suscitant de nombreuses
interprétations. L'interprétation dominante est celle
d'une loi physique stricte, ce qui impliquerait qu'un gain d'ordre d'un
côté
doit se payer par un désordre plus grand de l'autre, exprimant
une totale confusion avec le premier principe de conservation
de l'énergie (et pas de l'entropie!). L'interprétation
orthodoxe, qui est celle de Boltzmann, est d'ordre statistique, ce
qui est tout autre chose. Il peut donc y avoir diminution locale de
l'entropie (sinon rien ne pourrait jamais exister) même si ce
n'est pas le plus probable et qu'il n'y aura jamais diffusion de chaleur du froid
vers le chaud mais seulement du chaud vers le froid. Il est certain que
plus l'entropie est
faible, plus il est probable qu'elle
augmente, mais plus elle est grande (proche de l'équilibre) et
plus elle a des chances de diminuer (<i>création d'ordre à
partir du désordre</i>). Cette entropie négative est ce qu'on appelle l'auto-organisation. Il faut ajouter à cela
la fonction de l'information qui permet à la vie de lutter
contre l'entropie par une <i>création d'ordre à partir de
l'ordre</i>,
réussissant à répandre partout et rendre probable le
plus improbable : la vie elle-même. A cause du rôle qu'y joue l'information, la comparaison de la vie avec
un
réfrigérateur ou d'autres structures dissipatives n'est pas du tout pertinente. De même, si le
phénomène de "décohérence" qui implique une
perte d'information à chaque interaction au niveau quantique
peut être considéré comme son fondement physique,
on ne peut y réduire l'entropie dont le caractère
statistique et macroscopique reste essentiel, témoignant de sa dimension
subjective, "anthropique" (tout comme le concept d'information). C'est
ce qu'il faut souligner, la finalité pratique du concept
d'entropie et son horizon humain.<br>
<br>
  <br>
  <b>L'entropie n'est pas l'énergie</b> (pertes et gains d'entropie)<br>
  <br>
  <blockquote><small>On pourrait ainsi comparer un être vivant à un réfrigérateur: ce
dernier, tout en consommant de l'énergie de façon dissipative, retire
de la chaleur d'une source froide pour la donner à une source plus
chaude. De même, les êtres vivants consomment de l'énergie de façon
dissipative, pour accroître leur «ordre» (structures dans le temps et
l'espace, différences de potentiel, etc.) tout en rejetant de la
matière et de l'énergie «désorganisées», et le système pris dans son
ensemble voit son entropie augmenter.<br>
    </small>
    <div align="right"><a href="http://fr.encyclopedia.yahoo.com/articles/na/na_2293_p0.html"><small>http://fr.encyclopedia.yahoo.com/articles/na/na_2293_p0.html</small></a><br>
    </div>
  </blockquote>
  <br>Il
faudrait tordre le cou à cette assimilation de
l'entropie à l'<b>énergie</b>,
confusion qu'on trouve chez de
nombreux écologistes comme Georgescu-Roegen. Bien
qu'étant une loi d'une portée quasiment universelle,
l'entropie n'est pas un concept aussi fondamental que celui
d'énergie car il faut admettre la possibilité que
l'entropie diminue, qu'il y ait création (et perte)
d'information,
contrairement à l'énergie qui reste constante. En effet,
pour qu'il y ait entropie, dégradation, il faut qu'il y ait eu
création préalable d'ordre. Pas de mort sans vie. Cela
n'empêche pas
qu'il faut que nous consommions autant d'énergie que nous en
dépensons, mais le bilan entropique peut être positif, ne
serait-ce
que pour la raison qu'il y a d'énormes pertes d'énergie
qu'on peut éviter ou canaliser, il y a aussi des effets de seuil
qui font qu'une énergie minime au bon endroit et au bon moment
peut décider d'une bifurcation massive (miracle ou catastrophe).
On peut gagner au moins en efficacité
énergétique, ce qui est un gain net. La confusion entre
énergie et entropie vient du fait qu'en thermodynamique il y
a effectivement transfert d'entropie entre un système
ordonné qui interagit avec un système
désordonné (entre froid et chaud), comme s'il y avait un
transfert
d'énergie proportionnel, pourtant la chaleur n'est pas un flux
malgré
ce que suggère
l'expérience, c'est une agitation thermique, l'énergie
cinétique des atomes qui se propage et s'amortit. En dehors de
la thermodynamique la notion d'ordre est relative, voire
subjective et largement dépendante de l'échelle de temps
considéré.<br>
  <br>La
thermodynamique est une science ouvertement "anthropique", liée
à notre position d'observateur puisque c'est la science des
transformations de l'énergie au niveau
macroscopique, c'est-à-dire humain. C'est la science du
changement et donc du temps, l'entropie mesurant son
caractère irréversible. Pourtant, on ne peut identifier
complètement le temps à l'entropie qui détruit
toute chose car on ne pourrait expliquer alors qu'il y ait quelque
chose plutôt
que rien. Le temps est créateur au contraire car le simple <b>refroidissement</b>
de l'univers provoque
des "brisures de symétrie" comme la
cristallisation d'un
liquide, une eau qui devient soudain glace (par réduction de
l'agitation, du bruit). Le temps irréversible est au moins dans
l'expansion de
l'univers et son refroidissement qu'on peut dire entropique puisqu'il
pourrait nous mener à la mort thermique (ce qui n'est absolument
pas sûr). Ainsi l'entropie elle-même
est un facteur d'organisation et le temps un processus de
complexification presque autant que de dégradation. Le temps ou
l'entropie ne sont
pas
seulement destructeurs et homogénéisants mais produisent
aussi des événements improbables, des
différenciations, des
constructions de constructions, évolutions qui prennent du temps
(le temps est fractal, plus l'échelle est grande, plus il faut
du temps, pour la constitution d'amas de galaxies par exemple beaucoup
plus lente que pour les galaxies elles-mêmes).<br>
  <br>
Il y a enfin les
phénomènes
  <b>chaotiques</b> qui sont des sources de bifurcations, de structures
dissipatives, de création
d'ordre à partir du désordre :<br>
  <br>
  <small>Pour <a href="http://www.mpiwg-berlin.mpg.de/staff/segal/thesis/thesehtm/chap9/ch9c/ch9ctxt.htm">Shaw</a>
 la propriété de sensibilité aux conditions 
    initiales se trouve tout à fait secondaire dans la définition 
    des phénomènes chaotiques : cela obscurcit le trait essentiel 
    de la turbulence qui est la génération continue d’information intrinsèque 
    au flux lui-même</small><br>

  <br> On voit que l'entropie
n'est absolument pas comparable à l'énergie car il y a toujours pertes et
créations d'ordre alors qu'il n'y a jamais création
d'énergie.

Refuser toute possibilité de <b>création</b>
d'information ou d'ordre est impensable. Comment l'entropie et la
destruction
pourraient-ils régner en maître s'il n'y avait pas
création préalable, qui ne se limite pas au Big bang
où il n'y avait pas grand chose
en dehors de la chaleur ? On a vu qu'il suffit de faire intervenir
l'entropie
elle-même, le rayonnement, le refroidissement, pour expliquer des
brisures de symétrie (cristallisation, bifurcations, effets de
seuil ou d'échelle) qui sont des créations d'ordre et d'informations
(diversification). Cela ne veut pas dire qu'il y aurait
équivalence entre ordre et désordre, gains et pertes, on
n'est plus dans l'équivalence, ni même dans les
probabilités car toute existence reste un <a href="http://jeanzin.fr/ecorevo/sciences/miracle.htm">improbable miracle</a>,
ce qui ne l'empêche pas d'exister véritablement, de naître et de
mourir.<br>
  <br>
Sauf sans doute dans le domaine thermodynamique de production
de l'<b>énergie</b>,
le bilan de l'opération n'est pas
forcément nul, chaque progrès ne devant pas
obligatoirement être compensé par une dégradation
équivalente, car on n'est plus dans des processus
linéaires et l'information peut faire des miracles quotidiens.
Il y a une spécificité de l'entropie thermodynamique par
rapport à la simple notion d'ordre, c'est d'avoir affaire
directement à l'énergie (qui ne se crée pas mais
se perd...). Un des points que je voudrais souligner ici, c'est la
nécessité de
distinguer l'entropie énergétique de l'entropie qu'on
peut dire informationnelle (ou organisationnelle ou systémique)
et qui s'opposent comme la thermodynamique microscopique et
macroscopique.<br>

    <br>L'enjeu de cette distinction entre énergie et entropie, c'est ce qu'illustre le "démon de Maxwell",
capable de <b>trier</b>
les particules selon leurs particularités pour
inverser l'entropie statistique qui tend inexorablement vers une
moyenne sinon. Ce qui parait impossible au niveau
énergétique, nécessitant l'existence d'un
invraisemblable "démon", est pourtant exactement ce que
réalise la vie grâce à l'information (la
perception mais aussi les enzymes, cf. J. Monod).
Le caractère anti-entropique de l'information et de sa finalité biologique se
manifeste
par sa persistance dans un monde chaotique
et destructeur justement.<br>
  <br>
  <small>On peut dire que "<i>l'ordre réussit mieux que le désordre</i>"
mais ce n'est pas vraiment exact car le désordre résulte
lui-même
de lois physiques implacables, d'un ordre sous-jacent. Par contre il
est
vrai que dans le chaos permanent seul ce qui dure peut retenir notre
attention.
Sous cette forme, c'est une tautologie : seul ce qui dure persiste dans
l'être
! alors qu'il y a une dialectique entre ordre et désordre. La
vie
serait au moins un ordre de niveau 3 émergeant d'un
désordre
chaotique de niveau 2 produit par l'ordre immuable des lois physiques
au
niveau 1. On voit que l'entropie dépend du niveau auquel on se
situe, ce n'est pas une notion tout-à-fait objective.<br>
  </small>
  <br>
  <br>
  <b>L'entropie est une probabilité statistique</b> (égalisation des températures)<br>
  <br>Dans les années 1890, Ludwig Boltzmann
(1844-1906) a dû admettre face aux objections de Loschmidt et Zermelo que la
décroissance
de l'entropie n'était pas impossible mais seulement <b>improbable</b> ("Boltzmann a dû reconnaître que son théorème ne disait pas <i>l'impossibilité</i>
d'une évolution qui ferait décroître
spontanément l'entropie et contreviendrait donc au second
principe de thermodynamique, mais seulement <i>son improbabilité</i>",
Isabelle Stengers, Cosmopolitiques V, 2, p117). Boltzmann a même
reconnu que ce caractère statistique impliquait une fluctuation
de l'entropie (c'est de là qu'Einstein est parti pour mettre en
évidence l'existence des atomes dans le mouvement brownien). Si
la croissance de
l'entropie résultait d'une loi mécanique (des collisions
entre atomes) une inversion de la flèche du temps devrait
permettre de revenir à l'état antérieur, ce qui
est impensable (une tasse cassée ne se reconstitue jamais). Quel
que soit le sens de l'évolution elle se fera
toujours vers l'équilibre, vers l'état le plus probable,
comme si seules importaient les
vitesses des particules qui s'annulent et non leurs positions (comme
l'exigerait la dynamique). Pour expliquer l'entropie il faut
donc faire intervenir une
perte d'information à chaque collision ou interaction, un oubli
des conditions
initiales (ou pour suivre le Prigogine de 1962, le fait qu'il y a des
composantes négligeables, au regard des composantes principales,
dont l'influence est donc perdue. C'est un effet de seuil semble-t-il).
Le retour
à l'équilibre
thermodynamique, est une loi probabiliste et non pas mécanique,
passage du déséquilibre à l'équilibre
résultant de l'interaction d'un grand nombre de particules ou du
relâchement d'une contrainte (causalité négative).
Rien ne s'oppose à sa transgression mais rien ne la justifie non
plus. En
l'absence d'une nouvelle contrainte, il n'y a pas de raisons qu'il n'y
ait pas une égalisation, un mélange, un retour à
la moyenne. L'entropie résulte
du hasard, des grands nombres (d'Avogadro pour les gaz), de moyennes
statistiques, et n'a pas d'autre nécessité physique
impérieuse, ce
pourquoi
elle peut tout-à-fait diminuer lorsqu'une force même
très faible l'organise et creuse les
différences, sans que cela entraîne un désordre
équivalent ailleurs. C'est à la fois une loi implacable
(irréversible) qu'on
retrouve partout (tout système isolé perd de
l'énergie utilisable, réduit son différentiel
d'énergie), mais c'est aussi bien une loi statistique qui se
laisse contredire assez
facilement par une intervention extérieure, notamment par
l'information et les organismes biologiques qui savent
en tirer parti.<br>


  <br>
La thermodynamique, comme physique macroscopique, est une "physique
statistique" ou une "mécanique statistique"
même si elle prétend se fonder sur une théorie
  <b>réductionniste</b> mécaniste et donc sur les propriétés
des
particules. Depuis Boltzmann, la chaleur n'est plus assimilée
à un flux mais à l'agitation des atomes, ce qui lui
confère ses propriétés statistiques et l'oppose
à l'écoulement d'un liquide. Ce caractère
statistique ne peut être
éliminé, constituant un tout nouvel objet qui introduit
de façon indélébile le point de vue de
l'observateur humain et la notion d'information.<br>
  <br>
  <blockquote><small>La pression d'un gaz s'identifie à une
force moyenne exercée par les molécules en raison de leur
mouvement et de leurs chocs sur les parois. La température
s'interprète comme la manifestation macroscopique de
l'agitation désordonnée des constituants microscopiques,
et la chaleur comme l'énergie cinétique associée.
Le premier principe résulte alors de la conservation à
l'échelle microscopique de l'énergie mécanique.
[...] Le passage d'une échelle à l'autre entraîne
une modification qualitative des concepts et des
propriétés. La microphysique est discontinue,
probabiliste, linéaire, réversible, alors que la physique
macroscopique qui en découle est continue, déterministe,
non linéaire, irréversible.<br>
    </small><div align="right"><small>Roger Balian, p212</small><br>
    </div>
  </blockquote>
<br>Reste à expliquer la <b>constante</b> de Boltzmann (K) qui n'est pas du même type que les autres constantes (<i>h</i>,
c et G) et n'est véritablement valable que pour les gaz,
n'exprimant qu'une
simple conversion d'unité entre Joule et Kelvin,
l'énergie nécessaire pour augmenter la température
de 1° Kelvin (voir Atlan p175), le coût de cette information,
la constante de Boltzmann ayant été calculée
à partir de cette "constante des gaz parfaits" R=K<sub>B</sub>N<sub>A</sub> (où K<sub>B</sub> est la constante de Boltzmann et N<sub>A</sub> le Nombre d'Avogrado). Pourtant on a voulu lui 

donner le
sens plus général de "coefficient de
proportionnalité"
de l'entropie, ou quantum d'information,
qui permet de calculer l'entropie S. Pour Einstein elle exprime la stabilité thermique d'un
système à travers ses fluctuations d'énergie :<br>
  <br>
  <div align="center">S = k log W<br>
  </div>
  <br>
W est le
nombre
de complexions, de variables indépendantes, de degrés de
liberté, la <b>probabilité</b> thermodynamique de l'état du système, c'est-à-dire
le très grand nombre d'arrangements microscopiques (positions et vitesses de
toutes les particules) possibles pour un état macroscopique donné (par
ex., température et volume). Ainsi, dans un cristal, le nombre
d'arrangements possibles pour les atomes est beaucoup plus petit que
dans un gaz (cette description de l'entropie a été utilisée de façon
généralisée en mécanique quantique, où l'on doit tenir compte, pour
calculer W, du principe d'exclusion de Pauli).<br>
  <br>
  <div align="center">k = 1,38.10<sup>-23</sup> JK<sup>-1</sup><br>
  </div>

  <br>
  <small>JK signifie Joule par Kelvin, unité d'entropie
adaptée à notre échelle. "Sa petitesse va de pair
avec l'immensité des valeurs de W, qui vaut 10<sup>3.10<sup>22</sup></sup> lorsque S = 1 JK<sup>1</sup>. Puisque 1/k est de l'ordre du nombre d'Avogadro 6,022*10<sup>23</sup>mol<sup>-1</sup>,
l'incertitude I = S/k sur une système macroscopique est de
l'ordre de grandeur du nombre de ses constituants
élémentaires." Roger Balian, p213 (curieusement il donne pour valeur de k =<small> </small>0,96.10<sup>-23</sup> JK<sup>-1</sup>)</small><br>
<br>
Malgré son caractère statistique, l'entropie pose la question de l'<b>irréversibilité</b>
du temps mais ce n'est pas ce qui la fonde car le temps
est déjà irréversible dans toute trajectoire,
toute dynamique, toute évolution continuant le chemin parcouru.
L'ordre des causes est toujours le même, le 
temps a donc toujours la même direction. C'est la réversibilité qui est un mythe, sous
prétexte que les équations de la dynamique sont
réversibles, ce qui est une conséquence du premier
principe, de l'équivalence de l'effet à sa cause, mais on
ne peut s'en tenir à ce monde immobile figé comme la
flêche de Zénon qui n'atteint jamais sa cible.
L'irréversibilité est dans le Big Bang s'il a eu lieu,
dans le refroidissement de l'univers et son expansion, dans le
déploiement de ses constructions. De plus, l'entropie n'est pas
toujours irréversible. Il faut distinguer entropie interne ou
externe. Pour toute augmentation d'entropie (dS) on a dS&nbsp;= d<span class="heytta">S</span>e + dS<span class="heytta"></span>i, combinant entropie externe qui peut être réversible (<span class="heytta">S</span>e) et "génération d'entropie" interne irréversible (<span class="heytta">S</span>i)
dont s'occupe la thermodynamique des processus irréversibles. Il
n'empêche qu'une
tasse qui se casse ne revient jamais en arrière, un organisme
qui meurt ne revivra plus. L'irréversibilité entropique
traduit le passage à un état de plus grande
probabilité, ce qui peut être dû aux
frottements, aux
échanges ou à une simple déliaison (si on
enlève une cloison derrière laquelle un gaz était
confiné, il n'a plus aucune raison de rester dans son
état séparé, il est libéré dans
l'atmosphère et son entropie augmente. C'est irréversible
pour autant que la perte de la contrainte initiale est elle-même
irréversible). L'entropie mesure
cette perte
irréparable qui n'est pas toujours catastrophique quand elle
réduit les inégalités sociales par exemple.<br>
  <br>
Il
faut se méfier de l'utilisation trop rapide de concepts
physiques comme l'entropie dans un pathos déplacé qui
mélange tout. On peut souhaiter plus ou moins d'entropie
thermodynamique selon qu'il fait trop chaud ou trop froid ! Le bilan
"entropique" dépend des <b>finalités</b>
de celui qui le fait,
de ce qu'il observe, de la durée prise en compte et de la
situation d'entropie forte ou faible de
départ (plus l'entropie est basse et plus l'énergie est
récupérable).
Une complexification ou une diversification peut apparaître comme
un gain ou une perte d'entropie selon l'échelle
considérée, selon qu'on évalue le résultat
brut de l'extérieur ou qu'on se situe au niveau des formules
génératives, des dynamiques structurantes (un livre peut
paraître purement aléatoire ou très
structuré selon qu'on sait lire ou pas). De plus les
mesures sont souvent problématiques. Les atteintes
à la biodiversité par exemple, ne sont
pas une donnée aussi objective qu'un transfert de chaleur. Il
faut signaler enfin qu'il y a aussi une
irréversibilité liée au caractère subjectif
de l'entropie, à la perte d'information par son
récepteur, à cause de sa "rationalité
limitée" ou de problèmes de communication.<br>
  <br>
<br>
<small>
  </small><b>L'information c'est le contraire de l'entropie</b><small><br>
  <br></small>
  <blockquote><small>It follows from this that the idea of dissipation of energy depends
on the extent of our knowledge. Available energy is energy which we can
direct into any desired channel. Dissipated energy is energy which we cannot
lay hold of and direct at pleasure, such as the energy of the confused
agitation of molecules which we call heat. Now, confusion, like the correlative
term order, is not a property of material things in themselves, but only
in relation to the mind which perceives them.  A memorandum-book does not,
provided it is neatly written, appear confused to an illiterate person,
or to the owner who understands it thoroughly, but to any other person
able to read it appears to be inextricably confused. Similarly the notion
of dissipated energy could not occur to a being who could not turn any
of the energies of nature to his own account, or to one who could trace
the motion of every molecule and seize it at the right moment. It is only
to a being in the intermediate stage, who can lay hold of some forms of
energy while others elude his grasp, that energy appears to be passing
inevitably from the available to the dissipated state. (J.C.M.)</small>
    <div align="right"><small>James Clerk Maxwell, "<a href="http://jeckyll.uoregon.edu/%7Ewalston/Diffusion.html">Diffusion</a>", Encyclopaedia Brittanica,
1878</small><br>
    </div>
    <br>
    <small>Entropie, manque d'information, incertitude,
désordre, complexité, apparaissent donc comme des avatars
d'un
seul et même concept. Sous l'une ou l'autre de ces formes,
l'entropie
est associée à la notion de probabilité [...] Elle
caractérise non pas un objet en soi, mais la connaissance que
nous en avons et nos possibilités de faire des
prévisions. Elle a donc un caractère à la fois
objectif et subjectif.<br>

    
    </small><div align="right"><small>Roger Balian, Université de tous les savoirs, p220<br>

    </small></div><small>
  </small></blockquote>


  <br>
Curieusement, la notion d'entropie thermodynamique qui rend compte du
phénomène on ne peut plus physique de transformation de
la chaleur en travail, va bientôt se trouver identifiée
avec celle d'information à partir de la statistique et des
probabilités (Fischer, Shannon). La valeur d'une information étant
dans son improbabilité, il est effectivement strictement équivalent de
parler d'entropie ou de quantité d'information. Sauf que
l'information, c'est le <b>contraire</b>
de l'entropie comme improbabiluté, c'est même ce qui
permet de s'opposer à l'entropie pour une finalité
subjective, un
système cognitif, un organisme biologique. Après la confusion de l'entropie avec
l'énergie ou bien une loi mécanique qui ne soit pas statistique, la
confusion avec l'information mène paradoxalement à la
négation du caractère subjectif et anti-entropique de
l'information, si ce n'est à la confusion des
caractéristiques de la matière et de l'information qu'on
en
tire, frisant souvent l'absurde quand ce n'est pas un nouvel
idéalisme.<br>
  <br>On ne peut nier qu'il y a bien égalité stricte, à
l'inversion du signe près, entre l'entropie et la
quantité d'information. L'entropie peut effectivement se
définir comme l'information
perdue, le désordre, la multiplication des configurations
possibles, des irrégularités, la perte de forces
structurantes à longue
portée et le manque d'information qui en résulte. Quand
l'entropie augmente, notre information diminue. La raison de cette
équivalence est bien donnée par Maxwell mais
n'apparaît pas assez pourtant dans cette physique des effets
macroscopiques :
c'est que l'entropie implique déjà un observateur, un
point de vue <b>subjectif</b> constituant
son objet et mesurant sa dégradation, niveau
d'observation pour lequel l'information fait sens. Si l'entropie est déjà une notion subjective
d'ordre statistique, une probabilité macroscopique, l'identification avec l'information va plus loin puisqu'elle implique
qu'on ne s'intéresse à l'entropie que pour lutter contre !<br><br>En effet, l'information est le contraire de l'entropie par ses capacités
de reproduction, de répétition mais surtout de <b>régulation</b>,
c'est-à-dire de rétroaction et de correction d'erreurs
(la pertinence de
l'information
consiste à déclencher une réponse conditionnelle
adaptée). Par les régulations, la reproduction,
l'apprentissage (répétition) et l'organisation,
l'information est un élément de base fondamental
du vivant et de sa capacité de résistance à
l'entropie, de parer à l'imprévu, de corriger le
tir, de s'adapter après-coup grâce à un pilotage
par objectifs plutôt que par une programmation ou une cause
mécanique.
Le principe d'une boucle de rétroaction (ou d'un thermostat)
c'est de se régler sur l'écart entre le résultat
mesuré et la cible. On n'est plus alors dans la physique
mais dans le pilotage par objectif introduisant la finalité
dans la chaîne des causes et rejoignant les régulations
biologiques. Alors que la physique est le domaine de
la causalité (où les causes ont des effets), le
domaine de l'information ou de la vie est celui des régulations
et des finalités (où les effets deviennent causes).
On n'est plus dans une "obligation de moyens" mais
dans une "obligation de résultat" vitale qui nous oblige
à corriger nos erreurs. C'est ce qu'on peut appeler cultiver son
jardin.<br>
<small>
  <br>
  </small>Si l'information est bien le contraire de l'entropie, elle
est donc inséparable de <b>finalités </b>anti-entropiques,
c'est-à-dire de nos finalités humaines les plus
concrètes. Il est bien
difficile de comprendre ce concept d'information qui se
révèle autant subjectif
qu'objectif, puisque, si l'information doit renvoyer à un
phénomène objectif et que sa valeur est dans son
improbabilité, il n'y a d'information que
constituée par le
récepteur, il n'y a d'information que pour un système
cognitif, il n'y a d'information qu'en vue d'une action qui nous
préserve de l'entropie (ce qui peut être une
définition du travail, ce pourquoi il faut travailler pour
vivre). L'information c'est "<i>une différence qui fait la différence</i>"
(Bateson), elle n'est pas passive. Mieux, l'information nous oblige et nous rend responsables de
l'entropie justement. Les informations qui nous parviennent déterminent
nos responsabilités et c'est notamment à la mesure des informations
que nous avons sur les menaces écologiques (d'entropie) que nous pouvons y
remédier et que nous devenons responsables de l'avenir des
équilibres planétaires (nous devons corriger nos erreurs).<br>
  <br>En fin de compte, l'information nous permet de jouer le rôle du "démon de
Maxwell" inversant l'entropie par son intervention. Leo Szilard avait déjà fait le rapprochement en
1929,
montrant que "<i>le démon de Maxwell ne parvient à faire
décroître l'entropie du gaz que grâce à des
informations en sa possession</i>"
(p218). Ce rôle de l'information décisif dans le monde de
la vie, au niveau macroscopique, se révèle beaucoup plus limité au
niveau microscopique, surtout dans le domaine
énergétique, c'est-à-dire dans le domaine
strictement thermodynamique... En 1950, Léon Brillouin
calculera le <b>coût</b> énergétique minimum de toute
information, selon le principe de Gabor (<i>No free lunch</i>) qu'<i>on n'a rien
pour rien, même pas une observation</i> ! C'est ce qui permet de
déterminer "<i>l'efficience d'une expérience, par le rapport entre l'information acquise sur le coût entropique</i>".<br>

  <br>

  
  <div align="justify"><font size="-1" face="Times New Roman">“Un des résultats les plus 
    importants de la théorie de l’information appliquée à la physique 
    est la preuve selon laquelle il est impossible de déterminer l’état 
    d’un système avec une précision infinie”, puisqu'alors il faudrait une énergie infinie.<br>
</font>
  <div align="right"><a href="http://www.mpiwg-berlin.mpg.de/staff/segal/thesis/thesehtm/chap5/ch5b.htm"><font size="-1" face="Times New Roman">http://www.mpiwg-berlin.mpg.de/staff/segal/thesis/thesehtm/chap5/ch5b.htm</font></a><br>
  </div>
  </div>

  <font size="-1" face="Times New Roman"><br></font>Il
est important de souligner que l'information n'a pas d'effet en
elle-même, qu'elle doit se transformer en travail pour lutter
contre l'entropie, et donc dépenser de l'énergie, ce qui
augmente forcément l'entropie mais sans aucun rapport avec le
gain d'entropie, avec la valeur d'un travail qui peut être fait
en pure perte ou tout changer. Il n'y a aucune proportionnalité ici,
et ce qui constitue
une limitation de tout effort anti-entropique au niveau quantique ne
doit pas être assimilé à ce qui se passe au niveau
macroscopique où le coût de l'acquisition de l'information
est souvent sans commune mesure avec les <b>économies</b> qu'elle peut procurer.<br>

  <br>

  <small>Cependant, il faut être conscient que malgré la formidable efficience
de l'information, on doit s'attendre à rencontrer,
comme dans la physique des hautes énergies, une limite à cette
efficience et un épuisement de l'information au-delà d'un certain
seuil (trop d'information tue l'information). On peut parler d'un </small><small><b>coût croissant</b></small><small>
  de l'information. La loi des 90-10 (ou 80-20) énonce qu'il faut
10% du temps pour avoir 90% de l'information et donc 90% du temps pour obtenir 
les 10% restant. On peut même dire que ce coût est exponentiel 
lorsqu'on s'approche des 100%. L'information sera toujours ce qui manque, dans sa surabondance même.<br>
  <br>
  </small><small>
L'information est une question physique en tant qu'elle exprime une
interaction, qui n'est significative physiquement qu'au niveau quantique où
elle détermine un horizon indépassable, la limite de ce
qu'on peut savoir. </small><small>L'information exprime une interaction. La
constante de Planck h
est interprétée comme la quantité
minimale d'interaction
et la constante de Ludwig Boltzmann K comme le coût minimal d’une
information (en fait le rapport entre quantum d'énergie en Joule
et quantum de température de Kelvin, l'énergie qu'il faut
pour élever la température d'un degré à
pression et volume constant, or la température, l'agitation des
molécules, c'est l'entropie de Clausius). On comprend qu'on retrouve
K dans la formule de Shannon mesurant la quantité d'information
puisque c'est basé sur le coût électrique d'une
information transmise.La statistique quantique est basée sur ces deux
constantes h et K alors que la théorie du champ quantique est basée
sur h et c, comme la relativité générale sur
G et c. La longueur de Planck, à la base de la théorie des
cordes unifie h, c et G.<br>

  </small>
  
<br>

  <b>Entropie et décohérence</b> (pertes d'informations)<br>

  <br>On a vu qu'entropie
et information, l'une étant le contraire de l'autre, ne sont pas
du tout du même ordre que l'énergie puisqu'ils
intègrent un caractère subjectif, cognitif, probabiliste.
On a vu qu'il pouvait y avoir création d'information continue
dans les phénomènes chaotiques, les bifurcations et
brisures de symétrie mais pour sortir d'un déterminisme
laplacien implacable, il faut bien admettre la possibilité de
constantes pertes d'informations au plus bas niveau, de forces qui
s'enlisent et se perdent dans une interaction ou dans le bruit de fond
environnant limitant leur portée. Le formalisme physique
implique pourtant qu'aucune information ne serait jamais perdue ! "<i>En relativité (non quantique) comme en physique quantique à
l'échelon microscopique, aucune information ne peut jamais être
perdue</i>"<small> 20</small>. Cela paraît incroyable, comme s'il n'y
avait jamais aucune singularité (ou catastrophe) ! Il y a bien sûr
quantité de <b>pertes</b>
d'informations
(par le bruit ou la fureur). On a même bien du mal
à cause de cela à tester
"l'inséparabilité" au niveau quantique. C'est ce qu'on appelle le
phénomène de "décohérence", notion
signifiant simplement une perte
d'information (de superposition). A cette
entropie quantique, Hawking ajoute une entropie gravitationnelle moins
assurée (perte d'information dans un trou noir et rayonnement
thermodynamique). L'important est d'admettre le caractère
intenable d'un monde idéalisé sans pertes d'information,
où il ne se passe jamais rien. Tout événement,
toute interaction détruit de l'information et en crée
puisque c'est un changement de forme !<br>
  <br>
Lorsque Planck a découvert les quanta, il cherchait à
relier entropie et énergie, à fonder l'entropie sur une
propriété physique, l'<b>interaction</b>
de la matière
avec le rayonnement, et pas du
tout à rendre compte du caractère discontinu de
l'énergie qu'il a admis assez tardivement. On peut dire que la
physique quantique finira par lui donner raison en identifiant le
mécanisme de base de la perte d'information avec le
phénomène de décohérence.<br>
  <br>
  <blockquote><small>La théorie de la décohérence
a pour particularité de faire intervenir "l'environnement",
constitué de tout ce qui baigne les objets, par exemple l'air
dans lequel ils se propagent ou, si l'on fait le vide, le rayonnement
ambiant. Elle démontre que c'est leur interaction avec leur
environnement (système décrit par un nombre très
élevé de degrés de liberté) qui fait
très rapidement perdre aux objets macroscopiques leurs
propriétés quantiques. Tout se passe comme si des bribes
d'information sur leur état quantique s'échappait
continûment dans leur environnement. Ce dernier agit en somme
comme un observateur qui mesurerait les systèmes en permanence,
éliminant ainsi toutes les superpositions à
l'échelle macroscopique, donc aussi les interférences. Il
engendre bien une <i>décohérence</i>.<br>
    </small>
    <div align="right"><small>Etienne Klein, Petit voyage dans le monde des quanta, p163, Flammarion</small><br>
    </div>
  </blockquote>
<br>Répétons que ce qui ne se
perd
pas, et se conserve toujours, même en se dissipant sous forme de
rayonnement,
c'est l'<b>énergie</b>.
L'information (ou l'entropie) n'est
pas
réductible à l'énergie ni à une force car
elle
exprime une improbabilité, une différence, un
caractère
non linéaire (significatif) et n'a aucune
proportionnalité sinon statistique (probabilités). Il y a
donc bien conservation de l'énergie mais pas du tout des
caractéristiques de la matière (qui se transforme) ou de
l'information (ce qui ne voudrait rien dire car l'information change
avec le temps, elle s'use, il y a une entropie de l'information qui
perd sa pertinence). Tout cela ne signifie pas qu'il faudrait
réduire l'entropie à ce phénomène de
décohérence mais vise seulement à distinguer
l'énergie comme substance immuable de ses formes changeantes
plus ou moins éphémères ainsi que de l'information
qu'elle porte. L'entropie est bien réelle, et même
universelle, mais reste largement une
notion anthropique, macroscopique et (inter)subjective, tout comme
l'information dont la fonction dans une boucle de régulation est
de s'opposer à l'entropie par notre travail ou notre action
collective, au nom de nos finalités pratiques.<br>



  <br>Cette intrication de causes physiques et de points de vue
subjectifs (néanmoins objectifs) est difficile à concevoir mais c'est bien ce qui
permet de rendre compte de la vie et de la mort. De la "mort thermique"
qui nous menace sans cesse et de la vie qui la surmonte et dure
grâce à sa capacité d'utilisation de
l'énergie et de réaction aux informations qu'elle
prélève sur son environnement. La conclusion qu'on doit
en tirer est bien connue de tous, c'est que nous ne sommes pas condamnés à ne
rien faire et subir passivement car nous pouvons <b>changer</b>
l'avenir. Non
seulement nous pouvons, mais nous devons nous opposer à
l'entropie, aux ravages du temps, aux destructions écologiques,
en tirant parti des informations
disponibles pour nous organiser, reconstruire un ordre protecteur et
durable contre les
agressions extérieures en réaffirmant nos
finalités humaines et corrigeant nos erreurs. C'est une question
vitale, tâche
jamais achevée et toujours provisoire, la fragilité d'une
vie qui a démontré qu'elle peut résister à
tout pourtant, au moins tant qu'elle vit encore et ne se laisse pas
faire. C'est notre travail, notre responsabilité de lutte contre
l'entropie grâce aux informations disponibles.<br>
  <br><small>

Tout phénomène laissé à lui-même
va à sa perte selon les lois de l'entropie universelle.
C'est ce monde imparfait et fragile qui est entre nos mains et
auquel nous devons redonner sens. Il faut comprendre le monde
avant de le changer, manifester notre liberté vivante en
le sauvant de sa destruction et le rendre plus durable afin de
 <b>continuer</b> l'aventure humaine.
Nous devrons faire des miracles à hauteur des catastrophes annoncées.<br>
</small>
  <div align="right"><small><a href="http://jeanzin.fr/ecorevo/sciences/miracle.htm">L'improbable miracle d'exister</a></small></div>

</blockquote></div>
<br>

<div align="right"><small>Jean Zin 03/07/04<br>
<a href="http://jeanzin.fr/ecorevo/sciences/entropie.htm">http://jeanzin.fr/ecorevo/sciences/entropie.htm</a></small><br>
</div>

<hr width="100%" size="1" noshade="noshade"><a href="http://jeanzin.fr/ecorevo/">Index</a><br>


<br>

<br>

</body></html>